{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Neccesary packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#All neccesary classes for project\n",
    "\n",
    "#general\n",
    "import seaborn as sns\n",
    "import scipy.stats as ss\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import pickle\n",
    "import time\n",
    "import tensorflow as tf\n",
    "import shutil\n",
    "from datetime import datetime\n",
    "import os\n",
    "import random\n",
    "import sys\n",
    "import threading\n",
    "\n",
    "\n",
    "#for utility packages\n",
    "from Utilities.utilities import import_data\n",
    "from Utilities.utilities import DataFrameSelector\n",
    "from Utilities.utilities import CategoricalEncoder\n",
    "\n",
    "from functools import partial\n",
    "\n",
    "#image manipulation\n",
    "from PIL import Image as PI\n",
    "from PIL import ImageFilter\n",
    "from resizeimage import resizeimage\n",
    "import matplotlib.image as mpimg\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#Import Custom Functions\n",
    "from Utilities.model_builder import get_image\n",
    "from Utilities.model_builder import get_file_lists\n",
    "from Utilities.model_builder import parse_record\n",
    "from Utilities.model_builder import get_batch\n",
    "from Utilities.model_builder import build_iterator\n",
    "from Utilities.model_builder import get_values\n",
    "from Utilities.models import log_dir_build\n",
    "from Utilities.utilities import generate_image\n",
    "from Utilities.utilities import generate_image_array\n",
    "from Utilities.blur_tool import blur_images_in_directory\n",
    "from Utilities.resize_tool import resize_images_in_directory\n",
    "from Utilities.blur_tool import blur_and_print_image\n",
    "from Utilities.utilities import import_data\n",
    "from Utilities.utilities import DataFrameSelector\n",
    "from Utilities.utilities import CategoricalEncoder\n",
    "from Utilities.utilities import display_scores\n",
    "from Utilities.bounded_file_label_extractor import get_files_and_labels\n",
    "from Utilities.bounded_box_record_maker import process_bounded_image_files\n",
    "\n",
    "#for preprocessing\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import Imputer\n",
    "from sklearn.pipeline import FeatureUnion\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.cluster import KMeans"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import the data and split into training and validation data\n",
    "\n",
    "Here we are going to import the raw data and create a training and validation set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_data_csv = \"image_table_data.csv\"\n",
    "data_folder = \"D:/Machine_Learning/Datasets/corpus_learning\"\n",
    "image_folder = \"D:/Machine_Learning/Datasets/corpus_learning/raw_data/\"\n",
    "tf_record_folder = \"D:/Machine_Learning/Datasets/corpus_learning/tf_records/\"\n",
    " \n",
    "train_folder = \"D:/Machine_Learning/Datasets/corpus_learning/training_data/\"\n",
    "train_csv = \"D:/Machine_Learning/Datasets/corpus_learning/\" + \"train_data.csv\"\n",
    "val_folder = \"D:/Machine_Learning/Datasets/corpus_learning/validation_data/\"\n",
    "val_csv = \"D:/Machine_Learning/Datasets/corpus_learning/\" + \"val_data.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_data = import_data(raw_data_csv, data_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Split data into train and test sets\n",
    "train, test = train_test_split(raw_data, test_size=0.25, random_state=42)\n",
    "train = train.reset_index().drop(['index'], axis=1)\n",
    "test = test.reset_index().drop(['index'], axis=1)\n",
    "\n",
    "#print the CSV into desired folders\n",
    "train.to_csv(train_csv)\n",
    "test.to_csv(val_csv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [],
   "source": [
    "#move all corresponding images to correct test and validation folder\n",
    "\n",
    "#move training data\n",
    "for x in train['Filename']:\n",
    "    shutil.move(image_folder + x, train_folder + x)\n",
    "    \n",
    "#move validation data\n",
    "for x in test['Filename']:\n",
    "    shutil.move(image_folder + x, val_folder + x)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Make Tensorflow Records\n",
    "\n",
    "Here we are going to make tfrecords using the data we just divided up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = import_data(train_csv,'')\n",
    "val = import_data(val_csv, '')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_files, train_labels = get_files_and_labels(train, train_folder,'Filename')\n",
    "val_files, val_labels = get_files_and_labels(val, val_folder,'Filename')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Make train_records\n",
    "process_bounded_image_files(\"train\", train_files, train_labels, 2, 2, tf_record_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Launching 2 threads for spacings: [[0, 160], [160, 320]]\n",
      "2018-11-14 17:01:48.442719 [thread 1]: Wrote 160 images to D:/Machine_Learning/Datasets/corpus_learning/tf_records/validation-00001-of-00002\n",
      "2018-11-14 17:01:48.444720 [thread 1]: Wrote 160 images to 160 shards.\n",
      "2018-11-14 17:01:48.463720 [thread 0]: Wrote 160 images to D:/Machine_Learning/Datasets/corpus_learning/tf_records/validation-00000-of-00002\n",
      "2018-11-14 17:01:48.465719 [thread 0]: Wrote 160 images to 160 shards.\n",
      "2018-11-14 17:01:49.276720: Finished writing all 320 images in data set.\n"
     ]
    }
   ],
   "source": [
    "#Make test_records\n",
    "process_bounded_image_files(\"validation\", val_files, val_labels, 2, 2, tf_record_folder)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dev Area\n",
    "\n",
    "Use hts to test different ammounts of blur etc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
