{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "#All neccesary classes for project\n",
    "\n",
    "#general\n",
    "import seaborn as sns\n",
    "import scipy.stats as ss\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import pickle\n",
    "import time\n",
    "import sys\n",
    "\n",
    "#for preprocessing\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import Imputer\n",
    "from sklearn.pipeline import FeatureUnion\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.cluster import KMeans\n",
    "import tensorflow as tf\n",
    "\n",
    "#for machine learning\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from scipy.stats import expon, reciprocal\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.tree import export_graphviz\n",
    "\n",
    "#for evaluation\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "#for utility packages\n",
    "from Utilities.utilities import import_data\n",
    "from Utilities.utilities import DataFrameSelector\n",
    "from Utilities.utilities import CategoricalEncoder\n",
    "from Utilities.utilities import display_scores\n",
    "from Utilities.utilities import pipeline_transform\n",
    "from Utilities.utilities import reset_graph\n",
    "from Utilities.models import DNN_Model\n",
    "from Utilities.models import cross_val_score_dnn\n",
    "from functools import partial\n",
    "\n",
    "#image manipulation\n",
    "from PIL import Image as PI\n",
    "from resizeimage import resizeimage\n",
    "import matplotlib.image as mpimg\n",
    "import matplotlib.pyplot as plt\n",
    "from keras.utils.data_utils import get_file\n",
    "import imagenet_helper_files.vgg_preprocessing\n",
    "import tensorflow.contrib.slim as slim\n",
    "from tensorflow.contrib.slim.nets import resnet_v2\n",
    "import imagenet_helper_files.pnasnet as nas\n",
    "import imagenet_helper_files.nets.inception as inception\n",
    "\n",
    "#Import Custom Functions\n",
    "from Utilities.model_builder import get_image\n",
    "from Utilities.model_builder import get_file_lists\n",
    "from Utilities.model_builder import parse_record\n",
    "from Utilities.model_builder import get_batch\n",
    "from Utilities.model_builder import build_iterator\n",
    "from Utilities.bounded_model_builder import build_bounded_iterator\n",
    "from Utilities.model_builder import get_values_imagenet\n",
    "from Utilities.model_builder import get_values_bounded\n",
    "from Utilities.models import log_dir_build\n",
    "from Utilities.utilities import generate_image\n",
    "from Utilities.utilities import generate_image_array\n",
    "from Utilities.cell_net_predictor import Binary_Categorical_Predictor\n",
    "from Utilities.build_image_data_notebook import process_dataset\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load the Data\n",
    "\n",
    "Here we will load the training and validation data in order to do training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Build tf_records\n",
    "#Set Variables\n",
    "validation_directory = \"D:/Machine_Learning/Datasets/corpus_learning/validation\"\n",
    "train_directory = \"D:/Machine_Learning/Datasets/corpus_learning/train\"\n",
    "output_directory = \"D:/Machine_Learning/Datasets/corpus_learning/tf_records_2\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['D:/Machine_Learning/Datasets/corpus_learning/tf_records_2\\\\train-00000-of-00002',\n",
       " 'D:/Machine_Learning/Datasets/corpus_learning/tf_records_2\\\\train-00001-of-00002']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Import TFRecords for both Training and Testing of the Dat\n",
    "#Use the build_image_data.py to create these sets from your data\n",
    "\n",
    "train_list, val_list = get_file_lists(output_directory)\n",
    "train_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Test pulling a piece of data out of the set to ensure that records were created properly\n",
    "with tf.Session() as sess:\n",
    "    iterator_train = build_bounded_iterator(True, train_list, 1, num_epochs=1, num_parallel_calls=8, num_points = 8)\n",
    "    iterator_test = build_bounded_iterator(False, val_list, 1, num_epochs=1, num_parallel_calls=4, num_points = 8)\n",
    "    next_train = iterator_train.get_next()\n",
    "    next_test = iterator_test.get_next()\n",
    "    X_val, Y_val, file = next_train\n",
    "    X_val, Y_val, name = get_values_bounded(sess, X_val, Y_val, file, 37.5)\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_val "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#display an example and label\n",
    "\n",
    "x_val = X_val.reshape(331,331,3)\n",
    "y_val = Y_val[0]\n",
    "x_array = y_val[0:8:2] \n",
    "y_array = y_val[1:8:2]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "plt.imshow(x_val, interpolation='nearest')\n",
    "plt.scatter(x=x_array, y=y_array, c='r', s=40)\n",
    "plt.axis('on')\n",
    "plt.show()\n",
    "y_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "name\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build the Neural Network on top of Large Nas-Net\n",
    "\n",
    "Here we will build the Nas-Net and then stack our own network on top"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#import pnas_net model\n",
    "\n",
    "#Nasnet Model Location\n",
    "inception_net_model = 'D:/AI/models/inception_net/inception_v4.ckpt'\n",
    "landmark_ml_logs = 'D:/AI/models/landmark_ml/logs'\n",
    "landmark_ml_model = 'D:/AI/models/landmark_ml/model/landmark_ml_v3'\n",
    "landmark_ml_model_best = 'D:/AI/models/landmark_ml/model/landmark_ml_best_v3'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Reset the graph \n",
    "reset_graph()\n",
    "\n",
    "#Set constants for Neural Network\n",
    "dropout_rate = 0.2\n",
    "learning_rate = 0.1\n",
    "n_hidden1 = 2000\n",
    "#n_hidden2 = 1000\n",
    "n_hidden3 = 500\n",
    "#n_hidden4 = 100\n",
    "#n_hidden5 = 50\n",
    "n_final_layer = 8\n",
    "\n",
    "\n",
    "#Placeholder for input data\n",
    "X = tf.placeholder(tf.float32, shape=[None, 331, 331, 3], name=\"input\")\n",
    "y = tf.placeholder(tf.float32, shape=[None,8], name=\"bounding_box\")\n",
    "training = tf.placeholder_with_default(False, shape=(), name = 'training')\n",
    "\n",
    "#Define initalizer and batch normalization layers\n",
    "bn_batch_norm_layer = partial(tf.layers.batch_normalization, training=training, momentum=0.9)\n",
    "he_init = tf.contrib.layers.variance_scaling_initializer()\n",
    "\n",
    "\n",
    "#Import the Nas_Net and build it\n",
    "with slim.arg_scope(inception.inception_v4_arg_scope()):\n",
    "    net, end_points = inception.inception_v4(X, num_classes=1001, is_training=False)\n",
    "\n",
    "    #A saver to load the pretrained Data\n",
    "    saver = tf.train.Saver(name=\"Original_Saver\")\n",
    "\n",
    "    #For getting predictions from Original Network\n",
    "    inception_net_predictions = tf.get_default_graph().get_tensor_by_name(\"InceptionV4/Logits/Predictions:0\")\n",
    "\n",
    "\n",
    "    #Load in the noder where we are going to connect our own network\n",
    "    last_feature_node = tf.get_default_graph().get_tensor_by_name(\"InceptionV4/Logits/AvgPool_1a/AvgPool:0\")\n",
    "\n",
    "\n",
    "\n",
    "with tf.name_scope(\"Landmark_ML\"):\n",
    "    #Use a stop layer to freeze all the layers beneath in Nas-Net\n",
    "    with tf.name_scope(\"Isolate_Image_Features\"):\n",
    "        #get the output of the last cell layer  \n",
    "        features = tf.layers.flatten(last_feature_node)\n",
    "        #stop_layer = tf.stop_gradient(last_feature_node)\n",
    "        #starting_relu = tf.nn.relu(stop_layer, name=\"first_relu\")\n",
    "        #starting_relu = tf.nn.relu(last_feature_node, name=\"first_relu\")\n",
    "        #mean_pool = tf.reduce_mean(starting_relu, indices, name=\"condensing_mean\")\n",
    "            \n",
    "    \n",
    "    with tf.name_scope(\"Hidden_Layer_1\"):\n",
    "        hidden1 = tf.layers.dense(features, n_hidden1, name=\"hidden1\", kernel_initializer=he_init)\n",
    "        #hidden1 = tf.layers.dense(last_feature_node, n_hidden1, name=\"hidden1\", kernel_initializer=he_init)\n",
    "        hidden1_drop = tf.layers.dropout(hidden1, dropout_rate, training=training)\n",
    "        bn1 = bn_batch_norm_layer(hidden1_drop)\n",
    "        bn1_act = tf.nn.relu(bn1)\n",
    "    \n",
    "    \"\"\"\n",
    "    with tf.name_scope(\"Hidden_Layer_2\"):\n",
    "        hidden2 = tf.layers.dense(bn1_act, n_hidden2, name=\"hidden2\", kernel_initializer=he_init)\n",
    "        hidden2_drop = tf.layers.dropout(hidden2, dropout_rate, training=training)\n",
    "        bn2 = bn_batch_norm_layer(hidden2_drop)\n",
    "        bn2_act = tf.nn.relu(bn2)\n",
    "    \"\"\"\n",
    "\n",
    "    \n",
    "    with tf.name_scope(\"Hidden_Layer_3\"):\n",
    "        hidden3 = tf.layers.dense(bn1_act, n_hidden3, name=\"hidden3\", kernel_initializer=he_init)\n",
    "        hidden3_drop = tf.layers.dropout(hidden3, dropout_rate, training=training)\n",
    "        bn3 = bn_batch_norm_layer(hidden3_drop)\n",
    "        bn3_act = tf.nn.relu(bn3) \n",
    "        \n",
    "    \"\"\"    \n",
    "    with tf.name_scope(\"Hidden_Layer_4\"):\n",
    "        hidden4 = tf.layers.dense(bn3_act, n_hidden4, name=\"hidden4\", kernel_initializer=he_init)\n",
    "        hidden4_drop = tf.layers.dropout(hidden4, dropout_rate, training=training)\n",
    "        bn4 = bn_batch_norm_layer(hidden4_drop)\n",
    "        bn4_act = tf.nn.relu(bn4) \n",
    "\n",
    "    with tf.name_scope(\"Hidden_Layer_5\"):\n",
    "        hidden5 = tf.layers.dense(bn4_act, n_hidden5, name=\"hidden5\", kernel_initializer=he_init)\n",
    "        hidden5_drop = tf.layers.dropout(hidden5, dropout_rate, training=training)\n",
    "        bn5 = bn_batch_norm_layer(hidden5_drop)\n",
    "        bn5_act = tf.nn.relu(bn5) \n",
    "    \"\"\"\n",
    " \n",
    " \n",
    "    with tf.name_scope(\"Final_Layer\"):\n",
    "        box_estimates = tf.layers.dense(bn3_act, n_final_layer, name=\"outputs\")\n",
    "\n",
    "    \n",
    "    with tf.name_scope(\"loss\"):\n",
    "            loss = tf.losses.mean_squared_error(box_estimates, y)\n",
    "            loss_summary_test = tf.summary.scalar('loss_summary_test', loss)\n",
    "            loss_summary_train = tf.summary.scalar('loss_summary_train', loss)\n",
    "            \n",
    "    with tf.name_scope(\"train\"):\n",
    "        global_step = tf.Variable(0, trainable=False, name='global_step')\n",
    "        \n",
    "        #For momentum optomizer\n",
    "        \n",
    "        #decay_steps = 800\n",
    "        #decay_rate = 1/8\n",
    "        #learning_decay = tf.train.exponential_decay(learning_rate, global_step, decay_steps, decay_rate)\n",
    "        #optimizer = tf.train.MomentumOptimizer(learning_rate=learning_decay, momentum=0.9, use_nesterov=True)\n",
    "        \n",
    "        optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate)\n",
    "        training_op = optimizer.minimize(loss, global_step=global_step)        \n",
    "        \n",
    "            \n",
    "#Variables for global initialization\n",
    "saver2 = tf.train.Saver(name=\"Full_Saver\")\n",
    "init = tf.global_variables_initializer()\n",
    "extra_update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from D:/AI/models/inception_net/inception_v4.ckpt\n"
     ]
    }
   ],
   "source": [
    "#Initialize all variables and restore the lower layer\n",
    "with tf.Session() as sess:\n",
    "    #Initalizer all variables\n",
    "    init.run()\n",
    "    \n",
    "    #Restore the pretrained variables from Nas-Net\n",
    "    saver.restore(sess, inception_net_model)\n",
    "    \n",
    "    \n",
    "    #Save all of these variables to the new Cell_Net Model\n",
    "    saver2.save(sess, landmark_ml_model)\n",
    "    saver2.save(sess, landmark_ml_model_best)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#For Exporting Graph\n",
    "filewriter = tf.summary.FileWriter(landmark_ml_logs, tf.get_default_graph())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train the Network\n",
    "\n",
    "Here we are going to train the network. Accuracy/Loss is recorded\n",
    "Note for this version, a certain amount of the data is seen every training step. \n",
    "set train_size for how many images are trained on in each epoch\n",
    "set batch_size for how many images are trained at once.\n",
    "num_epochs is how many times the network sees that ammount of training data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the Graph into a file with Filewriter and add summaries for this session\n",
    "#This will be used for all the following\n",
    "model_path = log_dir_build(landmark_ml_logs, \"landmark_ml_v3\")\n",
    "filewriter = tf.summary.FileWriter(model_path, tf.get_default_graph())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from D:/AI/models/landmark_ml/model/landmark_ml_v3\n",
      "Loaded model. Training network initially. Logs into: D:/AI/models/landmark_ml/logs/landmark_ml_v3-run-20181119163104/\n",
      "Epoch: 0 Loss: 28707.1\n",
      "Epoch: 1Test Loss: 5551.73\n",
      "Epoch: 1Train Loss: 6261.17\n",
      "Epoch: 2Test Loss: 3618.33\n",
      "Epoch: 2Train Loss: 3960.58\n",
      "Epoch: 3Test Loss: 1541.32\n",
      "Epoch: 3Train Loss: 1820.18\n",
      "Epoch: 4Test Loss: 1475.16\n",
      "Epoch: 4Train Loss: 1308.99\n",
      "Epoch: 5Test Loss: 1458.22\n",
      "Epoch: 5Train Loss: 1510.47\n",
      "Epoch: 6Test Loss: 1006.01\n",
      "Epoch: 6Train Loss: 950.578\n",
      "Epoch: 7Test Loss: 835.899\n",
      "Epoch: 7Train Loss: 745.921\n",
      "Epoch: 8Test Loss: 1081.22\n",
      "Epoch: 8Train Loss: 985.042\n",
      "Epoch: 9Test Loss: 1173.7\n",
      "Epoch: 9Train Loss: 1072.23\n",
      "Epoch: 10Test Loss: 1070.32\n",
      "Epoch: 10Train Loss: 917.81\n",
      "Epoch: 11Test Loss: 1019.85\n",
      "Epoch: 11Train Loss: 579.059\n",
      "Epoch: 12Test Loss: 1149.92\n",
      "Epoch: 12Train Loss: 814.077\n",
      "Epoch: 13Test Loss: 837.844\n",
      "Epoch: 13Train Loss: 669.196\n",
      "Epoch: 14Test Loss: 713.168\n",
      "Epoch: 14Train Loss: 533.714\n",
      "Epoch: 15Test Loss: 647.98\n",
      "Epoch: 15Train Loss: 598.172\n",
      "Epoch: 16Test Loss: 663.598\n",
      "Epoch: 16Train Loss: 513.637\n",
      "Epoch: 17Test Loss: 719.492\n",
      "Epoch: 17Train Loss: 618.078\n",
      "Epoch: 18Test Loss: 696.11\n",
      "Epoch: 18Train Loss: 475.374\n",
      "Epoch: 19Test Loss: 667.262\n",
      "Epoch: 19Train Loss: 489.717\n",
      "Epoch: 20Test Loss: 621.562\n",
      "Epoch: 20Train Loss: 500.855\n",
      "Epoch: 21Test Loss: 615.797\n",
      "Epoch: 21Train Loss: 561.787\n",
      "Epoch: 22Test Loss: 648.359\n",
      "Epoch: 22Train Loss: 535.74\n",
      "Epoch: 23Test Loss: 728.395\n",
      "Epoch: 23Train Loss: 594.624\n",
      "Epoch: 24Test Loss: 700.153\n",
      "Epoch: 24Train Loss: 596.921\n",
      "Epoch: 25Test Loss: 837.86\n",
      "Epoch: 25Train Loss: 534.782\n",
      "Epoch: 26Test Loss: 683.896\n",
      "Epoch: 26Train Loss: 437.986\n",
      "Epoch: 27Test Loss: 686.747\n",
      "Epoch: 27Train Loss: 497.487\n",
      "Epoch: 28Test Loss: 724.646\n",
      "Epoch: 28Train Loss: 548.789\n",
      "Epoch: 29Test Loss: 607.312\n",
      "Epoch: 29Train Loss: 472.397\n",
      "Epoch: 30Test Loss: 798.255\n",
      "Epoch: 30Train Loss: 591.775\n",
      "Epoch: 31Test Loss: 658.807\n",
      "Epoch: 31Train Loss: 599.668\n",
      "Epoch: 32Test Loss: 671.461\n",
      "Epoch: 32Train Loss: 577.646\n",
      "Epoch: 33Test Loss: 895.294\n",
      "Epoch: 33Train Loss: 578.229\n",
      "Epoch: 34Test Loss: 578.872\n",
      "Epoch: 34Train Loss: 447.418\n",
      "Epoch: 35Test Loss: 737.899\n",
      "Epoch: 35Train Loss: 527.368\n",
      "Epoch: 36Test Loss: 721.502\n",
      "Epoch: 36Train Loss: 540.178\n",
      "Epoch: 37Test Loss: 678.691\n",
      "Epoch: 37Train Loss: 594.834\n",
      "Epoch: 38Test Loss: 623.547\n",
      "Epoch: 38Train Loss: 582.514\n",
      "Epoch: 39Test Loss: 731.038\n",
      "Epoch: 39Train Loss: 611.323\n",
      "Epoch: 40Test Loss: 557.348\n",
      "Epoch: 40Train Loss: 548.476\n",
      "Epoch: 41Test Loss: 699.722\n",
      "Epoch: 41Train Loss: 494.08\n",
      "Epoch: 42Test Loss: 785.569\n",
      "Epoch: 42Train Loss: 622.102\n",
      "Epoch: 43Test Loss: 751.983\n",
      "Epoch: 43Train Loss: 555.842\n",
      "Epoch: 44Test Loss: 619.582\n",
      "Epoch: 44Train Loss: 550.148\n",
      "Epoch: 45Test Loss: 605.554\n",
      "Epoch: 45Train Loss: 548.955\n",
      "Epoch: 46Test Loss: 587.388\n",
      "Epoch: 46Train Loss: 542.669\n",
      "Epoch: 47Test Loss: 490.706\n",
      "Epoch: 47Train Loss: 515.726\n",
      "Epoch: 48Test Loss: 534.585\n",
      "Epoch: 48Train Loss: 548.694\n",
      "Epoch: 49Test Loss: 667.125\n",
      "Epoch: 49Train Loss: 511.56\n",
      "Epoch: 50Test Loss: 647.257\n",
      "Epoch: 50Train Loss: 537.011\n",
      "Epoch: 51Test Loss: 551.75\n",
      "Epoch: 51Train Loss: 536.343\n",
      "Epoch: 52Test Loss: 710.84\n",
      "Epoch: 52Train Loss: 547.679\n",
      "Epoch: 53Test Loss: 750.871\n",
      "Epoch: 53Train Loss: 551.072\n",
      "Epoch: 54Test Loss: 573.567\n",
      "Epoch: 54Train Loss: 551.518\n",
      "Epoch: 55Test Loss: 714.39\n",
      "Epoch: 55Train Loss: 581.077\n",
      "Epoch: 56Test Loss: 730.354\n",
      "Epoch: 56Train Loss: 576.452\n",
      "Epoch: 57Test Loss: 794.365\n",
      "Epoch: 57Train Loss: 503.402\n",
      "Epoch: 58Test Loss: 577.038\n",
      "Epoch: 58Train Loss: 521.893\n",
      "Epoch: 59Test Loss: 827.996\n",
      "Epoch: 59Train Loss: 539.47\n",
      "Epoch: 60Test Loss: 782.762\n",
      "Epoch: 60Train Loss: 405.703\n",
      "Epoch: 61Test Loss: 639.17\n",
      "Epoch: 61Train Loss: 556.051\n",
      "Epoch: 62Test Loss: 514.272\n",
      "Epoch: 62Train Loss: 476.983\n",
      "Epoch: 63Test Loss: 758.511\n",
      "Epoch: 63Train Loss: 616.63\n",
      "Epoch: 64Test Loss: 677.167\n",
      "Epoch: 64Train Loss: 479.683\n",
      "Epoch: 65Test Loss: 754.343\n",
      "Epoch: 65Train Loss: 550.178\n",
      "Epoch: 66Test Loss: 590.859\n",
      "Epoch: 66Train Loss: 486.268\n",
      "Epoch: 67Test Loss: 677.408\n",
      "Epoch: 67Train Loss: 547.934\n",
      "Epoch: 68Test Loss: 781.626\n",
      "Epoch: 68Train Loss: 728.418\n",
      "Epoch: 69Test Loss: 667.132\n",
      "Epoch: 69Train Loss: 626.323\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-8-6df25eebcc16>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     44\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     45\u001b[0m             \u001b[1;31m#run Training Op\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 46\u001b[1;33m             \u001b[0msess\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mtraining_op\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mextra_update_ops\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m{\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mX_val\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0my_val\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtraining\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     47\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     48\u001b[0m         \u001b[1;31m#see if we are improving on the test data\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\sdgeo\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36mrun\u001b[1;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m    875\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    876\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[1;32m--> 877\u001b[1;33m                          run_metadata_ptr)\n\u001b[0m\u001b[0;32m    878\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    879\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\sdgeo\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run\u001b[1;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1098\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[1;32mor\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1099\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[1;32m-> 1100\u001b[1;33m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[0;32m   1101\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1102\u001b[0m       \u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\sdgeo\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_run\u001b[1;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1270\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1271\u001b[0m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[1;32m-> 1272\u001b[1;33m                            run_metadata)\n\u001b[0m\u001b[0;32m   1273\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1274\u001b[0m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\sdgeo\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_call\u001b[1;34m(self, fn, *args)\u001b[0m\n\u001b[0;32m   1276\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1277\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1278\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1279\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1280\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\sdgeo\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[1;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[0;32m   1261\u001b[0m       \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_extend_graph\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1262\u001b[0m       return self._call_tf_sessionrun(\n\u001b[1;32m-> 1263\u001b[1;33m           options, feed_dict, fetch_list, target_list, run_metadata)\n\u001b[0m\u001b[0;32m   1264\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1265\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\sdgeo\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_call_tf_sessionrun\u001b[1;34m(self, options, feed_dict, fetch_list, target_list, run_metadata)\u001b[0m\n\u001b[0;32m   1348\u001b[0m     return tf_session.TF_SessionRun_wrapper(\n\u001b[0;32m   1349\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1350\u001b[1;33m         run_metadata)\n\u001b[0m\u001b[0;32m   1351\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1352\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_call_tf_sessionprun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#For doing the initial training\n",
    "#Total number of epochs to train\n",
    "num_epochs = 200\n",
    "steps_between_test_save = 1\n",
    "batch_size = 15\n",
    "train_size = 1700\n",
    "all_data_steps = np.int(np.floor(train_size/batch_size))\n",
    "lowest_loss = 100\n",
    "offset_correction = 37.5\n",
    "\n",
    "#GPU Options\n",
    "config = tf.ConfigProto()\n",
    "config.gpu_options.allocator_type = 'BFC'\n",
    "config.gpu_options.allow_growth = True\n",
    "\n",
    "\n",
    "with tf.Session(config=config) as sess:\n",
    "    start_time = time.time()\n",
    "    #restore saver, build iterator, set the step to the global step\n",
    "    saver2.restore(sess, landmark_ml_model)\n",
    "    \n",
    "    #Set up the global steps\n",
    "    step = 0\n",
    "    print(\"Loaded model. Training network initially. Logs into: \" + model_path)\n",
    "    \n",
    "    iterator_train = build_bounded_iterator(True, train_list, batch_size, num_epochs=num_epochs, num_parallel_calls=8, num_points = 8)\n",
    "    iterator_test = build_bounded_iterator(False, val_list, batch_size, num_epochs=num_epochs, num_parallel_calls=4, num_points = 8)\n",
    "    next_train = iterator_train.get_next()\n",
    "    next_test = iterator_test.get_next()\n",
    "    \n",
    "    #Get initial loss\n",
    "    X_val, y_val, file = next_train\n",
    "    X_val, y_val, filename = get_values_bounded(sess, X_val, y_val, file, offset_correction)\n",
    "    loss_sum, loss_val = sess.run([loss_summary_train, loss], feed_dict = {X: X_val, y: y_val, training: False})\n",
    "    filewriter.add_summary(loss_sum, step)\n",
    "    print(\"Epoch: \" + str(step) + \" Loss: \" + str(loss_val))\n",
    "    \n",
    "    #Iterate through training \n",
    "    while step < num_epochs:\n",
    "        for i in range(all_data_steps):\n",
    "            #Get training data\n",
    "            X_val, y_val, file = next_train\n",
    "            X_val, y_val, filename = get_values_bounded(sess, X_val, y_val, file, offset_correction)\n",
    "        \n",
    "            #run Training Op\n",
    "            sess.run([training_op, extra_update_ops], feed_dict={X: X_val, y: y_val, training: True})\n",
    "        \n",
    "        #see if we are improving on the test data\n",
    "        #Maybe Test Accuracy\n",
    "        if ((step % steps_between_test_save) == 0 and step != 0) :\n",
    "            X_val, y_val, file = next_test\n",
    "            X_val, y_val, filename = get_values_bounded(sess, X_val, y_val, file, offset_correction)\n",
    "            loss_sum, loss_val = sess.run([loss_summary_test, loss], feed_dict = {X: X_val, y: y_val, training: False})\n",
    "            filewriter.add_summary(loss_sum, step)\n",
    "            print(\"Epoch: \" + str(step) + \"Test Loss: \" + str(loss_val))\n",
    "            \n",
    "            X_val, y_val, file = next_train\n",
    "            X_val, y_val, filename = get_values_bounded(sess, X_val, y_val, file, offset_correction)\n",
    "            loss_sum, loss_val = sess.run([loss_summary_train, loss], feed_dict = {X: X_val, y: y_val, training: False})\n",
    "            filewriter.add_summary(loss_sum, step)\n",
    "            print(\"Epoch: \" + str(step) + \"Train Loss: \" + str(loss_val))\n",
    "            saver2.save(sess, landmark_ml_model)\n",
    "            if lowest_loss > loss_val:\n",
    "                lowest_loss = loss_val\n",
    "                saver2.save(sess, landmark_ml_model_best)\n",
    "        step = step + 1\n",
    "            \n",
    "    #Finish the final Model\n",
    "    saver2.save(sess, landmark_ml_model)\n",
    "    end_time = time.time()\n",
    "    total_steps = tf.train.global_step(sess, global_step)\n",
    "    final_time = end_time - start_time\n",
    "    print(\"Did \" + str(total_steps) + \" of loss minimized training in \" + str(final_time) + \" seconds.\")\n",
    "    print(\"Done!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Do metric testing\n",
    "\n",
    "Here we will run through all of the training data and relate accuracy with confidence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out_of_set_net_model = 'D:/AI/models/out_of_set_net/model/out_of_set_net_best'\n",
    "batch_size = 1\n",
    "num_epochs = 1500\n",
    "\n",
    "#Set up frame\n",
    "column_list = ['Item_Number','Category','Category_Index','Category_Strength', 'Max_Confidence_Index_Value','Calculated_Confidence',\n",
    "                                       'Correct_Confidence','Confidence_Score']\n",
    "full_data_frame = pd.DataFrame(columns=column_list)\n",
    "\n",
    "\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    \n",
    "    #build the iterator with all the training data\n",
    "    iterator_test_run = build_iterator(False, val_list, 1, num_epochs=1500, num_parallel_calls=12)\n",
    "    next_test_get = iterator_test_run.get_next()\n",
    "    \n",
    "    #Get Saver Data\n",
    "    new_saver = tf.train.import_meta_graph(out_of_set_net_model + '.meta')\n",
    "    new_saver.restore(sess, out_of_set_net_model)\n",
    "    \n",
    "    #Set up environment for test\n",
    "    training = tf.get_default_graph().get_tensor_by_name(\"training:0\")\n",
    "    \n",
    "    \n",
    "    #Loop Through Test Data\n",
    "    \n",
    "    for i in range(num_epochs):\n",
    "        #get data\n",
    "        X_val, y_val = next_test_get\n",
    "        X_val, y_val = get_values(sess, X_val, y_val)\n",
    "\n",
    "\n",
    "        soft_max_pna = tf.get_default_graph().get_tensor_by_name(\"final_layer/predictions:0\")\n",
    "        soft_max_confidence = tf.get_default_graph().get_tensor_by_name(\"Out_Of_Set_Classifier/Final_Layer/final_soft_max:0\")\n",
    "        X = tf.get_default_graph().get_tensor_by_name(\"input:0\")\n",
    "\n",
    "        y_category = soft_max_pna.eval(feed_dict={X: X_val, training: False})\n",
    "        y_confidence = soft_max_confidence.eval(feed_dict={X: X_val, training: False})\n",
    "        y_confidence_correct = y_val[[0]]\n",
    "\n",
    "        #Append Data Frame with Requried Information\n",
    "        cat_max = np.argmax(y_category)\n",
    "        con_max = np.argmax(y_confidence)\n",
    "\n",
    "        case_number = i\n",
    "        item_name = image_net_dict_file.get(cat_max)\n",
    "        y_category_strength = y_category[[0],[cat_max]][0]\n",
    "        y_confidence_strength = y_confidence[[0],[con_max]][0]\n",
    "        y_confidence_correct = y_val[[0]][0]\n",
    "        confidence_score = y_confidence[[0],[0]][0]\n",
    "\n",
    "        to_add = pd.DataFrame([[case_number,item_name,cat_max,y_category_strength,con_max,y_confidence_strength,\n",
    "                            y_confidence_correct,confidence_score]], columns = column_list)\n",
    "        full_data_frame = full_data_frame.append(to_add)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Print out the dataframe\n",
    "\n",
    "full_data_frame.to_csv('Out_Of_Set_Test_Run_1.csv', sep=',')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Additional Testing\n",
    "\n",
    "Here are some helpful scripts for doing additional testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Location and models to use for testing\n",
    "\n",
    "nas_net_model = 'D:/AI/models/pnas_net/model.ckpt'\n",
    "landmark_ml_logs = 'D:/AI/models/landmark_ml/logs'\n",
    "landmark_ml_model = 'D:/AI/models/landmark_ml/model/landmark_ml_v3'\n",
    "landmark_ml_model_best = 'D:/AI/models/landmark_ml/model/landmark_ml_best_v3'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_image = PI.open('D:/Machine_Learning/Datasets/corpus_learning/test_image/final_brain.jpg')\n",
    "test_image = np.array(test_image)\n",
    "test_image = test_image.reshape(1,331,331,3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Test a single test image\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    saver2.restore(sess, landmark_ml_model)\n",
    "    \n",
    "    iterator_test = build_bounded_iterator(False, val_list, 1, num_epochs=1, num_parallel_calls=4, num_points = 8)\n",
    "    next_test = iterator_test.get_next()\n",
    "    \n",
    "    X_val, Y_val, file = next_test\n",
    "    X_val, Y_val, filename = get_values_bounded(sess, X_val, Y_val, file, offset_correction)\n",
    "    loss_val = sess.run([loss], feed_dict = {X: X_val, y: Y_val, training: False})\n",
    "    \n",
    "    \n",
    "    Y_estimate = box_estimates.eval(feed_dict={training: False, X: X_val})\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#display an example and label\n",
    "\n",
    "x_val = X_val.reshape(331,331,3)\n",
    "y_val = Y_estimate[0]\n",
    "x_array = y_val[0:8:2] \n",
    "y_array = y_val[1:8:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "plt.imshow(x_val, interpolation='nearest')\n",
    "plt.axis('on')\n",
    "plt.show()\n",
    "y_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "plt.imshow(x_val, interpolation='nearest')\n",
    "plt.scatter(x=x_array, y=y_array, c='r', s=40)\n",
    "plt.axis('on')\n",
    "plt.show()\n",
    "y_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.Session() as sess:\n",
    "    #restore graph from meta and restore variables\n",
    "    new_saver = tf.train.import_meta_graph(out_of_set_net_best + '.meta')\n",
    "    new_saver.restore(sess, out_of_set_net_best)\n",
    "    soft = tf.get_default_graph().get_tensor_by_name(\"Out_Of_Set_Classifier/Final_Layer/final_soft_max:0\")\n",
    "    input_tensor = tf.get_default_graph().get_tensor_by_name(\"input:0\")\n",
    "    val = soft.eval(feed_dict={input_tensor: cat, training: False})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "current_step\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_net_dict_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list = np.reshape(last_layers, (num_epochs,batch_size,4320))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.Session() as sess:\n",
    "    #restore saver, build iterator, set the step to the global step\n",
    "    saver2.restore(sess, out_of_set_net_model)\n",
    "    \n",
    "    #Set up the global steps\n",
    "    total_steps = tf.train.global_step(sess, global_step)\n",
    "    \n",
    "    print(\"Did \" + str(total_steps) + \" of loss minimized training in \" + str(final_time) + \" time.\")\n",
    "    print(\"Done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
